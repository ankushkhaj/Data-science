{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree,model_selection\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "# A convenience for displaying visualizations.\n",
    "from IPython.display import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    module=\"scipy\",\n",
    "    message=\"^internal gelsd\"\n",
    ")\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "start=datetime.now()\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:/Users/ankush/Desktop/2018_03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>train_id</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>from</th>\n",
       "      <th>from_id</th>\n",
       "      <th>to</th>\n",
       "      <th>to_id</th>\n",
       "      <th>scheduled_time</th>\n",
       "      <th>actual_time</th>\n",
       "      <th>delay_minutes</th>\n",
       "      <th>status</th>\n",
       "      <th>line</th>\n",
       "      <th>type</th>\n",
       "      <th>Delay_Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>3805</td>\n",
       "      <td>1.000</td>\n",
       "      <td>New York Penn Station</td>\n",
       "      <td>105</td>\n",
       "      <td>New York Penn Station</td>\n",
       "      <td>105</td>\n",
       "      <td>2018-03-02 01:22:00</td>\n",
       "      <td>2018-03-02 01:21:05</td>\n",
       "      <td>0.000</td>\n",
       "      <td>departed</td>\n",
       "      <td>Northeast Corrdr</td>\n",
       "      <td>NJ Transit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>3805</td>\n",
       "      <td>2.000</td>\n",
       "      <td>New York Penn Station</td>\n",
       "      <td>105</td>\n",
       "      <td>Secaucus Upper Lvl</td>\n",
       "      <td>38187</td>\n",
       "      <td>2018-03-02 01:31:00</td>\n",
       "      <td>2018-03-02 01:31:08</td>\n",
       "      <td>0.133</td>\n",
       "      <td>departed</td>\n",
       "      <td>Northeast Corrdr</td>\n",
       "      <td>NJ Transit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>3805</td>\n",
       "      <td>3.000</td>\n",
       "      <td>Secaucus Upper Lvl</td>\n",
       "      <td>38187</td>\n",
       "      <td>Newark Penn Station</td>\n",
       "      <td>107</td>\n",
       "      <td>2018-03-02 01:40:00</td>\n",
       "      <td>2018-03-02 01:40:07</td>\n",
       "      <td>0.117</td>\n",
       "      <td>departed</td>\n",
       "      <td>Northeast Corrdr</td>\n",
       "      <td>NJ Transit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>3805</td>\n",
       "      <td>4.000</td>\n",
       "      <td>Newark Penn Station</td>\n",
       "      <td>107</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>37953</td>\n",
       "      <td>2018-03-02 01:45:00</td>\n",
       "      <td>2018-03-02 01:45:10</td>\n",
       "      <td>0.167</td>\n",
       "      <td>departed</td>\n",
       "      <td>Northeast Corrdr</td>\n",
       "      <td>NJ Transit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>3805</td>\n",
       "      <td>5.000</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>37953</td>\n",
       "      <td>North Elizabeth</td>\n",
       "      <td>109</td>\n",
       "      <td>2018-03-02 01:49:00</td>\n",
       "      <td>2018-03-02 01:49:10</td>\n",
       "      <td>0.167</td>\n",
       "      <td>departed</td>\n",
       "      <td>Northeast Corrdr</td>\n",
       "      <td>NJ Transit</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date train_id  stop_sequence                   from  from_id  \\\n",
       "0  2018-03-01     3805          1.000  New York Penn Station      105   \n",
       "1  2018-03-01     3805          2.000  New York Penn Station      105   \n",
       "2  2018-03-01     3805          3.000     Secaucus Upper Lvl    38187   \n",
       "3  2018-03-01     3805          4.000    Newark Penn Station      107   \n",
       "4  2018-03-01     3805          5.000         Newark Airport    37953   \n",
       "\n",
       "                      to  to_id       scheduled_time          actual_time  \\\n",
       "0  New York Penn Station    105  2018-03-02 01:22:00  2018-03-02 01:21:05   \n",
       "1     Secaucus Upper Lvl  38187  2018-03-02 01:31:00  2018-03-02 01:31:08   \n",
       "2    Newark Penn Station    107  2018-03-02 01:40:00  2018-03-02 01:40:07   \n",
       "3         Newark Airport  37953  2018-03-02 01:45:00  2018-03-02 01:45:10   \n",
       "4        North Elizabeth    109  2018-03-02 01:49:00  2018-03-02 01:49:10   \n",
       "\n",
       "   delay_minutes    status              line        type  Delay_Category  \n",
       "0          0.000  departed  Northeast Corrdr  NJ Transit               0  \n",
       "1          0.133  departed  Northeast Corrdr  NJ Transit               1  \n",
       "2          0.117  departed  Northeast Corrdr  NJ Transit               1  \n",
       "3          0.167  departed  Northeast Corrdr  NJ Transit               1  \n",
       "4          0.167  departed  Northeast Corrdr  NJ Transit               1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Delay_Category']=np.where(data['delay_minutes']>0,1,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=data.drop(['Price','Date','Address','Propertycount'],axis=1)\n",
    "X=pd.get_dummies(data.drop(['scheduled_time', 'actual_time', 'delay_minutes'], 1))\n",
    "Y=data['Delay_Category']\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#X = StandardScaler().fit_transform(X)\n",
    "sm = SMOTE(random_state=2)\n",
    "X_sampled,y_sampled = sm.fit_sample(X,Y)\n",
    "X_train_sample,X_test_sample,y_train_sample,y_test_sample = train_test_split(X_sampled,y_sampled,test_size=.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification problem so we will now PCA for dimension reduction to get most of the variance in least variables as we can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of total variance in the dataset explained by each component from Sklearn PCA.\n",
      " [5.98084081e-01 4.01915788e-01 1.12982863e-07 1.47602342e-09\n",
      " 6.10182751e-10]\n"
     ]
    }
   ],
   "source": [
    "X_pca = PCA(n_components=5)\n",
    "X_pca.fit(X_train_sample)\n",
    "X_train_pca = X_pca.transform(X_train_sample)\n",
    "X_test_pca = X_pca.transform(X_test_sample)\n",
    "print(\n",
    "    'The percentage of total variance in the dataset explained by each',\n",
    "    'component from Sklearn PCA.\\n',\n",
    "    X_pca.explained_variance_ratio_\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected 5 components which are explaning most of the variance in the dataset.We will use all the classification models -Naive Bayes,KNN,Decision Tree,Random Forest, Linear Regression, Ridge Regression, Lasso Regression, SVC,GBR and fit the model with the data and check the accuracy of the model.First we will use gridsearch cv to get the best parameters and then the below techniques for each model and compare them-\n",
    "\n",
    "Cross Validation\n",
    "\n",
    "Classification_report\n",
    "\n",
    "AUC\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9446848401880841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB() \n",
    "#Fitting the model with the train data\n",
    "bnb = bnb.fit(X_train_pca, y_train_sample)\n",
    "y_pred_bnb = bnb.predict(X_train_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have fitted the model with the data and we dont have any parameters to tune using gridsearch cv so we are only using Cross Validation\n",
    "Classification_report,AUC,Confusion Matrix accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data accuracy for Naive Bayes:  [0.94476684 0.94551082 0.94580841 0.94473543 0.9425927  0.94396167\n",
      " 0.94419975 0.94616232 0.94520996 0.94390048]\n",
      "Testing data accuracy Naive Bayes:  [0.94417331 0.94572075 0.94667302 0.94703012 0.94453041 0.94381621\n",
      " 0.94345238 0.94511905 0.9452381  0.94238095]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data accuracy for Naive Bayes: \",cross_val_score(bnb, X_train_pca, y_train_sample, cv=10))\n",
    "print(\"Testing data accuracy Naive Bayes: \",cross_val_score(bnb, X_test_pca, y_test_sample, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC(Area Under the Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score for BNB : 0.9446707058518109\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print (\"AUC Score for BNB :\" ,roc_auc_score(y_train_sample, y_pred_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[155079  12818]\n",
      " [  5769 162354]]\n",
      "Accuracy Score for BNB : 0.9446848401880841\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "print(confusion_matrix(y_train_sample, y_pred_bnb))\n",
    "print('Accuracy Score for BNB :',accuracy_score(y_train_sample, y_pred_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for BNB:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.92      0.94    167897\n",
      "          1       0.93      0.97      0.95    168123\n",
      "\n",
      "avg / total       0.95      0.94      0.94    336020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report for BNB:\",classification_report(y_train_sample, y_pred_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.KNN Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'leaf_size': 1, 'n_neighbors': 5}, {'leaf_size': 2, 'n_neighbors': 5}, {'leaf_size': 3, 'n_neighbors': 5}, {'leaf_size': 5, 'n_neighbors': 5}]\n",
      "[0.99599726 0.99599726 0.99599726 0.99599726]\n",
      "[0.99734837 0.99734837 0.99734837 0.99734837]\n",
      "Best Hyper Parameters:\n",
      " {'leaf_size': 1, 'n_neighbors': 5}\n",
      "0.9959972620677341\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=1, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_pca, y_train_sample)\n",
    "y_pred_knn = knn_model.predict(X_train_pca)\n",
    "params = {'n_neighbors':[5],\n",
    "          'leaf_size':[1,2,3,5]\n",
    "                  }\n",
    "#Making models with hyper parameters sets\n",
    "grid_class_knn = model_selection.GridSearchCV(knn_model, param_grid=params)\n",
    "#Learning\n",
    "grid_class_knn.fit(X_train_pca,y_train_sample)\n",
    "#The best hyper parameters set\n",
    "\n",
    "results = grid_class_knn.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",grid_class_knn.best_params_)\n",
    "print(grid_class_knn.best_score_)\n",
    "final_model = grid_class_knn.best_estimator_\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data accuracy for Naive Bayes:  [0.9969348  0.99717287 0.99711335 0.99687519 0.99672639 0.99705375\n",
      " 0.99705375 0.99636916 0.99705366 0.99651796]\n",
      "Testing data accuracy Naive Bayes:  [0.98690632 0.98714439 0.98559695 0.98762052 0.98714439 0.98607309\n",
      " 0.99047619 0.98916667 0.99       0.98630952]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data accuracy for Naive Bayes: \",cross_val_score(grid_class_knn, X_train_pca, y_train_sample, cv=10))\n",
    "print(\"Testing data accuracy Naive Bayes: \",cross_val_score(grid_class_knn, X_test_pca, y_test_sample, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print (\"AUC Score for BNB :\" ,roc_auc_score(y_train_sample, y_pred_bnb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
