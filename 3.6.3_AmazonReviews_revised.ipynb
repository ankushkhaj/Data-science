{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6.3 Amazon Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.\n",
    "This dataset includes reviews (ratings, text, helpfulness votes), product metadata (descriptions, category information, price, brand, and image features), and links (also viewed/also bought graphs). The file is present in URL-http://jmcauley.ucsd.edu/data/amazon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns are as below:\n",
    "\n",
    "reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "\n",
    "asin - ID of the product, e.g. 0000013714\n",
    "\n",
    "reviewerName - name of the reviewer\n",
    "\n",
    "helpful - helpfulness rating of the review, e.g. 2/3\n",
    "\n",
    "reviewText - text of the review\n",
    "\n",
    "overall - rating of the product\n",
    "\n",
    "summary - summary of the review\n",
    "\n",
    "unixReviewTime - time of the review (unix time)\n",
    "\n",
    "reviewTime - time of the review (raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree,model_selection\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "# A convenience for displaying visualizations.\n",
    "from IPython.display import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Display preferences.\n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "import gzip\n",
    "import numpy as np\n",
    "import json\n",
    "# Suppress annoying harmless error.\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\",\n",
    "    module=\"scipy\",\n",
    "    message=\"^internal gelsd\"\n",
    ")\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "start=datetime.now()\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "#df = getDF('C:/Users/ankush/Downloads/reviews_Pet_Supplies_5.json.gz')\n",
    "df=data = pd.read_json('C:/Users/ankush/Desktop/Pet_Supplies_5.json',lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only care about only one column here-reviewText in the dataset.Then we will get the keywords out of the column reviewtext and will evaluate if they are negative or positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count   65430.000\n",
       "mean        4.245\n",
       "std         1.183\n",
       "min         1.000\n",
       "25%         4.000\n",
       "50%         5.000\n",
       "75%         5.000\n",
       "max         5.000\n",
       "Name: overall, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using countvector object here to first convert convert a collection of text documents to a matrix of token counts and then using tdidf to get the measured frequency of each token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65430, 45087)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#positive_keywords = ['great','enjoyed','loves','like','recommend','happy','yummy','nice','very','good','easy','useful','awesome','works','love','excellent','helpful','Best','nicely','wonderful']\n",
    "#negative_keywords = ['not','disgusting','junk','bad','didn\\'t work','waste','crap']\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(df.reviewText)\n",
    "X_counts.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65430, 45087)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [1 if x > 3 else 0 for x in df.overall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# Instantiate our model and store it in a new variable.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=2)\n",
    "X_sampled,y_sampled = sm.fit_sample(X_tfidf,Y)\n",
    "X_train_sample,X_test_sample,y_train_sample,y_test_sample = train_test_split(X_sampled,y_sampled,test_size=.20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Dimension Reduction SVD to reduce the features since PCA has limitation for being used in sparse matrix and then use each model on these techniques as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "X_svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "X_svd.fit(X_train_sample)  \n",
    "X_train_svd = X_svd.transform(X_train_sample)\n",
    "X_test_svd = X_svd.transform(X_test_sample)\n",
    "\n",
    "\n",
    "#X_pca = PCA(n_components=3)\n",
    "#X_pca.fit(X_train_sample)\n",
    "#X_train_pca = X_pca.transform(X_train_sample)\n",
    "#X_test_pca = X_pca.transform(X_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using PCA, we can see 10 features are closely correlated to the output variable so we will keep them.Now we will use these techniques for each Classification model\n",
    "#Gridsearchcv\n",
    "#Cross Validation\n",
    "#classification_report\n",
    "#AUC\n",
    "#Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7073962355212355\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes doesn't need parameter tuning so we just need to get only accuracy score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB() \n",
    "bnb = bnb.fit(X_train_svd, y_train_sample)\n",
    "y_pred_bnb = bnb.predict(X_train_svd)\n",
    "print(\"Accuracy:\", bnb.score(X_train_svd, y_train_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_svd, y_train_sample)\n",
    "y_pred_knn = knn_model.predict(X_train_svd)\n",
    "params = {'n_neighbors':[5,6,7,8,9,10],\n",
    "          'leaf_size':[1,2,3,5]\n",
    "                  }\n",
    "#Making models with hyper parameters sets\n",
    "grid_class = model_selection.GridSearchCV(knn_model, param_grid=params)\n",
    "#Learning\n",
    "grid_class.fit(X_train_svd,y_train_sample)\n",
    "#The best hyper parameters set\n",
    "\n",
    "results = grid_class.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",grid_class.best_params_)\n",
    "print(grid_class.best_score_)\n",
    "final_model = grid_class.best_estimator_\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'max_depth': 3, 'max_features': 50}, {'max_depth': 4, 'max_features': 50}, {'max_depth': 5, 'max_features': 50}]\n",
      "[0.75380068 0.76043678 0.76911197]\n",
      "[0.75500242 0.76358832 0.77361969]\n",
      "Best Hyper Parameters:\n",
      " {'max_depth': 5, 'max_features': 50}\n",
      "0.7691119691119691\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features=50, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "[0 0 1 ... 0 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "rfc = ensemble.RandomForestClassifier(max_depth=10,n_estimators= 100)\n",
    "rfc.fit(X_train_svd, y_train_sample)\n",
    "y_pred_rfc = rfc.predict(X_train_svd)\n",
    "#Gridsearchcv to get the best parameters\n",
    "dt_grid={'max_depth':[3,4,5],'max_features': [50]}\n",
    "\n",
    "grid_class_rfc=model_selection.GridSearchCV(rfc,dt_grid,cv=6)\n",
    "grid_class_rfc.fit(X_train_svd, y_train_sample)\n",
    "results = grid_class_rfc.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",grid_class_rfc.best_params_)\n",
    "print(grid_class_rfc.best_score_)\n",
    "final_model = grid_class_rfc.best_estimator_\n",
    "print(final_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'max_depth': 50, 'max_features': 10}]\n",
      "[0.76692809]\n",
      "[0.99979247]\n",
      "Best Hyper Parameters:\n",
      " {'max_depth': 50, 'max_features': 10}\n",
      "0.7669280888030888\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=50,\n",
      "            max_features=10, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "decision_tree = tree.DecisionTreeClassifier(max_depth=10)\n",
    "decision_tree.fit(X_train_svd, y_train_sample)\n",
    "y_pred_dt = decision_tree.predict(X_train_svd)\n",
    "dt_grid={'max_depth':[50],'max_features': [10]}\n",
    "grid_class_dt=model_selection.GridSearchCV(decision_tree,dt_grid,cv=6)\n",
    "grid_class_dt.fit(X_train_svd, y_train_sample)\n",
    "results = grid_class_dt.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",grid_class_dt.best_params_)\n",
    "print(grid_class_dt.best_score_)\n",
    "final_model = grid_class_dt.best_estimator_\n",
    "print(final_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "from sklearn.svm import SVC\n",
    "svm=SVC()\n",
    "svm.fit(X_train_pca, y_train)\n",
    "y_pred_svm = svm.predict(X_train_pca)\n",
    "param_grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}\n",
    "grid_class_svm=model_selection.GridSearchCV(SVC(),param_grid,refit = True)\n",
    "grid_class_svm.fit(X_train_pca, y_train)\n",
    "results = grid_class_svm.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",grid_class_svm.best_params_)\n",
    "print(grid_class_svm.best_score_)\n",
    "final_model = grid_class_svm.best_estimator_\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'C': 0.001, 'penalty': 'l1'}, {'C': 0.001, 'penalty': 'l2'}, {'C': 0.01, 'penalty': 'l1'}, {'C': 0.01, 'penalty': 'l2'}, {'C': 0.1, 'penalty': 'l1'}, {'C': 0.1, 'penalty': 'l2'}, {'C': 1.0, 'penalty': 'l1'}, {'C': 1.0, 'penalty': 'l2'}, {'C': 10.0, 'penalty': 'l1'}, {'C': 10.0, 'penalty': 'l2'}, {'C': 100.0, 'penalty': 'l1'}, {'C': 100.0, 'penalty': 'l2'}, {'C': 1000.0, 'penalty': 'l1'}, {'C': 1000.0, 'penalty': 'l2'}]\n",
      "[0.68929778 0.75611728 0.78295125 0.78559363 0.82481902 0.81934122\n",
      " 0.82825772 0.82747346 0.82839044 0.82835425 0.8284749  0.82839044\n",
      " 0.82846284 0.8284749 ]\n",
      "[0.6892589  0.75629692 0.78308129 0.78589125 0.8256864  0.82001421\n",
      " 0.82898702 0.82809417 0.82910634 0.82901518 0.82908623 0.82909159\n",
      " 0.82907148 0.82907953]\n",
      "Best Hyper Parameters:\n",
      " {'C': 100.0, 'penalty': 'l1'}\n",
      "0.8284749034749035\n",
      "LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression\n",
    "lr = LogisticRegression(C=1e9)\n",
    "lr.fit(X_train_svd, y_train_sample)\n",
    "y_pred_lr = lr.predict(X_train_svd)\n",
    "#Gridsearchcv \n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=model_selection.GridSearchCV(logreg,grid,cv=10)\n",
    "logreg_cv.fit(X_train_svd,y_train_sample)\n",
    "results = logreg_cv.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",logreg_cv.best_params_)\n",
    "print(logreg_cv.best_score_)\n",
    "final_model = logreg_cv.best_estimator_\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBR\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 7,\n",
    "          'loss': 'deviance'}\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train_pca, y_train)\n",
    "y_pred_gbr = clf.predict(X_train_pca)\n",
    "\n",
    "params ={\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"n_estimators\":[1000]\n",
    "    }\n",
    "gb = model_selection.GridSearchCV(clf, params, cv=10)\n",
    "gb.fit(X_train, y_train)\n",
    "results = gb.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",gb.best_params_)\n",
    "print(gb.best_score_)\n",
    "final_model = gb.best_estimator_\n",
    "print(final_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchcv  looks to be good for all the models except Decision Tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data accuracy for Naive Bayes:  [0.70659911 0.70310049 0.70141151 0.70756424 0.71187259 0.70921815\n",
      " 0.70821769 0.70001207 0.70882105 0.7094244 ]\n",
      "Testing data accuracy Naive Bayes:  [0.72503618 0.72310661 0.70815244 0.69126869 0.70994208 0.70366795\n",
      " 0.71559633 0.71994206 0.71849348 0.70787059]\n",
      "Training data accuracy for RandomForest:  [0.80757631 0.82338038 0.81891664 0.82880927 0.82142857 0.82215251\n",
      " 0.82502715 0.82309642 0.82201038 0.82261373]\n",
      "Testing data accuracy for RandomForest:  [0.81186686 0.80607815 0.80221901 0.81283164 0.81418919 0.80984556\n",
      " 0.81265089 0.81506519 0.82665379 0.83631096]\n",
      "Training data accuracy for Decision Tree:  [0.75763059 0.768609   0.76390397 0.77246954 0.76749517 0.77099421\n",
      " 0.77591408 0.77181127 0.764209   0.77808616]\n",
      "Testing data accuracy for Decision Tree:  [0.74722624 0.72841293 0.74433189 0.74529667 0.75048263 0.71573359\n",
      " 0.73297924 0.73442781 0.75229358 0.75132786]\n",
      "Training data accuracy for Linear Regression :  [0.81626252 0.83701291 0.82386295 0.83701291 0.82782336 0.83083977\n",
      " 0.82382044 0.83419814 0.82864728 0.82526849]\n",
      "Testing data accuracy for Linear Regression  :  [0.82296189 0.82392668 0.82682103 0.82296189 0.82818533 0.81805019\n",
      " 0.82858522 0.82568807 0.84451956 0.84017383]\n",
      "Training data accuracy for BNB:  [0.70659911 0.70310049 0.70141151 0.70756424 0.71187259 0.70921815\n",
      " 0.70821769 0.70001207 0.70882105 0.7094244 ]\n",
      "Testing data accuracy for BNB:  [0.72503618 0.72310661 0.70815244 0.69126869 0.70994208 0.70366795\n",
      " 0.71559633 0.71994206 0.71849348 0.70787059]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes model \n",
    "print(\"Training data accuracy for Naive Bayes: \",cross_val_score(bnb, X_train_svd, y_train_sample, cv=10))\n",
    "print(\"Testing data accuracy Naive Bayes: \",cross_val_score(bnb, X_test_svd, y_test_sample, cv=10))\n",
    "#KNN\n",
    "\n",
    "#print(\"Training data accuracy for KNN: \",cross_val_score(knn_model, X_train_pca, y_train, cv=10))\n",
    "#print(\"Testing data accuracy for KNN: \",cross_val_score(knn_model, X_test_pca, y_test, cv=10))\n",
    "#Random Forest\n",
    "print(\"Training data accuracy for RandomForest: \",cross_val_score(rfc, X_train_svd, y_train_sample, cv=10))\n",
    "print(\"Testing data accuracy for RandomForest: \",cross_val_score(rfc, X_test_svd, y_test_sample, cv=10))\n",
    "#Decision Tree\n",
    "print(\"Training data accuracy for Decision Tree: \",cross_val_score(decision_tree, X_train_svd, y_train_sample, cv=10))\n",
    "print(\"Testing data accuracy for Decision Tree: \",cross_val_score(decision_tree, X_test_svd, y_test_sample, cv=10))\n",
    "#SVC\n",
    "#print(\"Training data accuracy for SVC: \",cross_val_score(SVC(), X_train_pca, y_train, cv=6))\n",
    "#print(\"Testing data accuracy for SVC: \",cross_val_score(SVC(), X_test_pca, y_test, cv=6))\n",
    "#Linear Regression\n",
    "print(\"Training data accuracy for Linear Regression : \",cross_val_score(lr, X_train_svd, y_train_sample, cv=10))\n",
    "print(\"Testing data accuracy for Linear Regression  : \",cross_val_score(lr, X_test_svd, y_test_sample, cv=10))\n",
    "#GBR\n",
    "#print(\"Training data accuracy for GBR: \",cross_val_score(clf, X_train_pca, y_train, cv=10))\n",
    "#print(\"Testing data accuracy for GBR: \",cross_val_score(clf, X_test_pca, y_test, cv=10))\n",
    "\n",
    "print(\"Training data accuracy for BNB: \",cross_val_score(bnb, X_train_svd, y_train_sample, cv=10))\n",
    "print(\"Testing data accuracy for BNB: \",cross_val_score(bnb, X_test_svd, y_test_sample, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It actually looks like we did good on cross val with PCA than we did with our full feature set. There is no overfitting since we have derived our own features with the keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area Under Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score for BNB : 0.7068965376279065\n",
      "AUC Score for Random Forest: 0.8650449992624996\n",
      "AUC Score for Decision Tree: 0.8132196050081972\n",
      "AUC Score for Linear Regression: 0.8292603488036231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print (\"AUC Score for BNB :\" ,roc_auc_score(y_train_sample, y_pred_bnb))\n",
    "#print (\"AUC Score for KNN:\" ,roc_auc_score(y_train, y_pred_knn))\n",
    "print (\"AUC Score for Random Forest:\" ,roc_auc_score(y_train_sample,y_pred_rfc))\n",
    "print (\"AUC Score for Decision Tree:\" ,roc_auc_score(y_train_sample, y_pred_dt))\n",
    "#print (\"AUC Score for SVM:\" ,roc_auc_score(y_train, y_pred_svm))\n",
    "print (\"AUC Score for Linear Regression:\" ,roc_auc_score(y_train_sample, y_pred_lr))\n",
    "#print (\"AUC Score for GBR:\" ,roc_auc_score(y_train, y_pred_gbr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC performed ok with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33419  7905]\n",
      " [ 3267 38289]]\n",
      "Accuracy Score for Random Forest : 0.8652027027027027\n",
      "[[34254  7070]\n",
      " [ 8414 33142]]\n",
      "Accuracy Score for Decision Tree : 0.8131756756756757\n",
      "[[34642  6682]\n",
      " [ 7471 34085]]\n",
      "Accuracy Score for Linear Regression : 0.8292350386100386\n",
      "[[29316 12008]\n",
      " [12285 29271]]\n",
      "Accuracy Score for BNB : 0.7068894787644787\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "#print(confusion_matrix(y_train, y_pred_bnb))\n",
    "#print('Accuracy Score for KBN :',accuracy_score(y_train, y_pred_bnb))\n",
    "#print(confusion_matrix(y_train, y_pred_knn))\n",
    "#print('Accuracy Score for KNN :',accuracy_score(y_train, y_pred_knn))\n",
    "print(confusion_matrix(y_train_sample, y_pred_rfc))\n",
    "print('Accuracy Score for Random Forest :',accuracy_score(y_train_sample, y_pred_rfc))\n",
    "print(confusion_matrix(y_train_sample, y_pred_dt))\n",
    "print('Accuracy Score for Decision Tree :',accuracy_score(y_train_sample, y_pred_dt))\n",
    "#print(confusion_matrix(y_train, y_pred_SVM))\n",
    "#print('Accuracy Score for SVM:',accuracy_score(y_train, y_pred_svm))\n",
    "print(confusion_matrix(y_train_sample, y_pred_lr))\n",
    "print('Accuracy Score for Linear Regression :',accuracy_score(y_train_sample, y_pred_lr))\n",
    "#print(confusion_matrix(y_train, y_pred_gbr))\n",
    "#print('Accuracy Score for GBR :',accuracy_score(y_train, y_pred_gbr))\n",
    "print(confusion_matrix(y_train_sample, y_pred_bnb))\n",
    "print('Accuracy Score for BNB :',accuracy_score(y_train_sample, y_pred_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy looks to be doing well after PCA.There are 3 models KNN,SVC,GBR  that are taking lot of time to complete.Accuracy is similar but these 3 models are very slow compared to others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for RFC:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.81      0.86     41324\n",
      "          1       0.83      0.92      0.87     41556\n",
      "\n",
      "avg / total       0.87      0.87      0.86     82880\n",
      "\n",
      "Classification Report for DT:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.83      0.82     41324\n",
      "          1       0.82      0.80      0.81     41556\n",
      "\n",
      "avg / total       0.81      0.81      0.81     82880\n",
      "\n",
      "Classification Report for LR:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.84      0.83     41324\n",
      "          1       0.84      0.82      0.83     41556\n",
      "\n",
      "avg / total       0.83      0.83      0.83     82880\n",
      "\n",
      "Classification Report for BNB:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.71      0.71     41324\n",
      "          1       0.71      0.70      0.71     41556\n",
      "\n",
      "avg / total       0.71      0.71      0.71     82880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "#print(\"Classification Report for KNN:\",classification_report(y_train, y_pred_knn))\n",
    "print(\"Classification Report for RFC:\",classification_report(y_train_sample, y_pred_rfc))\n",
    "print(\"Classification Report for DT:\",classification_report(y_train_sample, y_pred_dt))\n",
    "#print(\"Classification Report for SVM:\",classification_report(y_train, y_pred_svm))\n",
    "print(\"Classification Report for LR:\",classification_report(y_train_sample, y_pred_lr))\n",
    "#print(\"Classification Report for GBR:\",classification_report(y_train, y_pred_gbr))\n",
    "print(\"Classification Report for BNB:\",classification_report(y_train_sample, y_pred_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score and recall are very good and ran at a good time.Now,lets get the accuracy using same techniques after going feature reduction SelectKbest(SKB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selectkbest(SKB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [0 0 0 ... 0 0 0] are constant.\n",
      "  UserWarning)\n",
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "skb = SelectKBest(f_classif,k=100)\n",
    "skb.fit(X_train_sample, y_train_sample)\n",
    "X_train_skb = skb.transform(X_train_sample)\n",
    "X_test_skb = skb.transform(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearchcv for KNN\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_skb, y_train_sample)\n",
    "y_pred_knn = knn_model.predict(X_train_skb)\n",
    "params = {'n_neighbors':[5],\n",
    "          'leaf_size':[1,2,3,5]\n",
    "                  }\n",
    "#Making models with hyper parameters sets\n",
    "grid_class = model_selection.GridSearchCV(knn_model, param_grid=params)\n",
    "#Learning\n",
    "grid_class.fit(X_train_skb,y_train_sample)\n",
    "#The best hyper parameters set\n",
    "\n",
    "results = grid_class.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",grid_class.best_params_)\n",
    "print(grid_class.best_score_)\n",
    "final_model = grid_class.best_estimator_\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'max_depth': 3, 'max_features': 100}, {'max_depth': 4, 'max_features': 100}, {'max_depth': 5, 'max_features': 100}]\n",
      "[0.71914817 0.73122587 0.74121622]\n",
      "[0.7207336  0.73363418 0.74592906]\n",
      "Best Hyper Parameters:\n",
      " {'max_depth': 5, 'max_features': 100}\n",
      "0.7412162162162163\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features=100, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Gridsearchcv for Random Forest\n",
    "rfc = ensemble.RandomForestClassifier(max_depth=10,n_estimators= 100)\n",
    "rfc.fit(X_train_skb, y_train_sample)\n",
    "y_pred_rfc = rfc.predict(X_train_skb)\n",
    "\n",
    "#Gridsearchcv to get the best parameters\n",
    "dt_grid={'max_depth':[3,4,5],'max_features': [100]}\n",
    "\n",
    "grid_class_rfc=model_selection.GridSearchCV(rfc,dt_grid,cv=6)\n",
    "grid_class_rfc.fit(X_train_skb, y_train_sample)\n",
    "results = grid_class_rfc.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",grid_class_rfc.best_params_)\n",
    "print(grid_class_rfc.best_score_)\n",
    "final_model = grid_class_rfc.best_estimator_\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'max_depth': 50, 'max_features': 100}]\n",
      "[0.77502413]\n",
      "[0.999264]\n",
      "Best Hyper Parameters:\n",
      " {'max_depth': 50, 'max_features': 100}\n",
      "0.7750241312741313\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=50,\n",
      "            max_features=100, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Gridsearchcv for Decision Tree\n",
    "decision_tree = tree.DecisionTreeClassifier(max_depth=50)\n",
    "decision_tree.fit(X_train_skb, y_train_sample)\n",
    "y_pred_dt = decision_tree.predict(X_train_skb)\n",
    "dt_grid={'max_depth':[50],'max_features': [100]}\n",
    "grid_class_dt=model_selection.GridSearchCV(decision_tree,dt_grid,cv=6)\n",
    "grid_class_dt.fit(X_train_skb, y_train_sample)\n",
    "results = grid_class_dt.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",grid_class_dt.best_params_)\n",
    "print(grid_class_dt.best_score_)\n",
    "final_model = grid_class_dt.best_estimator_\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearchcv for SVC\n",
    "from sklearn.svm import SVC\n",
    "svm=SVC()\n",
    "svm.fit(X_train_skb, y_train)\n",
    "y_pred_svm = svm.predict(X_train_pca)\n",
    "param_grid = {'C':[1,10,100,1000],'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}\n",
    "grid_class_svm=model_selection.GridSearchCV(SVC(),param_grid,refit = True)\n",
    "grid_class_svm.fit(X_train_skb, y_train_sample)\n",
    "results = grid_class_svm.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",grid_class_svm.best_params_)\n",
    "print(grid_class_svm.best_score_)\n",
    "final_model = grid_class_svm.best_estimator_\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'C': 0.001, 'penalty': 'l1'}, {'C': 0.001, 'penalty': 'l2'}, {'C': 0.01, 'penalty': 'l1'}, {'C': 0.01, 'penalty': 'l2'}, {'C': 0.1, 'penalty': 'l1'}, {'C': 0.1, 'penalty': 'l2'}, {'C': 1.0, 'penalty': 'l1'}, {'C': 1.0, 'penalty': 'l2'}, {'C': 10.0, 'penalty': 'l1'}, {'C': 10.0, 'penalty': 'l2'}, {'C': 100.0, 'penalty': 'l1'}, {'C': 100.0, 'penalty': 'l2'}, {'C': 1000.0, 'penalty': 'l1'}, {'C': 1000.0, 'penalty': 'l2'}]\n",
      "[0.50054295 0.74416023 0.74854006 0.76478041 0.80591216 0.79517375\n",
      " 0.80798745 0.80694981 0.80815637 0.80815637 0.80814431 0.80815637\n",
      " 0.80814431 0.80812017]\n",
      "[0.50054295 0.74415889 0.74919965 0.76492117 0.80648461 0.79591377\n",
      " 0.80887494 0.80773943 0.80910419 0.80904654 0.80915647 0.80914441\n",
      " 0.80915513 0.80915111]\n",
      "Best Hyper Parameters:\n",
      " {'C': 10.0, 'penalty': 'l1'}\n",
      "0.8081563706563707\n",
      "LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankush\\anacondanew\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Gridsearchcv for Linear Regression\n",
    "lr = LogisticRegression(C=1e9)\n",
    "lr.fit(X_train_skb, y_train_sample)\n",
    "y_pred_lr = lr.predict(X_train_skb)\n",
    "#Gridsearchcv \n",
    "grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "logreg=LogisticRegression()\n",
    "logreg_cv=model_selection.GridSearchCV(logreg,grid,cv=10)\n",
    "logreg_cv.fit(X_train_skb,y_train_sample)\n",
    "results = logreg_cv.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",logreg_cv.best_params_)\n",
    "print(logreg_cv.best_score_)\n",
    "final_model = logreg_cv.best_estimator_\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearchcv for GBR\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 7,\n",
    "          'loss': 'deviance'}\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train_skb, y_train_sample)\n",
    "y_pred_gbr = clf.predict(X_train_skb)\n",
    "\n",
    "params ={\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"n_estimators\":[1000]\n",
    "    }\n",
    "gb = model_selection.GridSearchCV(clf, params, cv=10)\n",
    "gb.fit(X_train_skb, y_train_sample)\n",
    "results = gb.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",gb.best_params_)\n",
    "print(gb.best_score_)\n",
    "final_model = gb.best_estimator_\n",
    "print(final_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearchcv for all the models look good.There is no overfitting as well.This shows the best parameters and these parameters are actually good but these 3 models-KNN,SVC,GBR are very slow to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data accuracy for Naive Bayes:  [0.70671975 0.71793944 0.70406563 0.70394499 0.70901194 0.71195849\n",
      " 0.7102691  0.70821769 0.71702667 0.70749366]\n",
      "Testing data accuracy Naive Bayes:  [0.70670526 0.72165943 0.7105644  0.72021225 0.71490593 0.70642202\n",
      " 0.71269918 0.70980203 0.7136649  0.71704491]\n",
      "Training data accuracy for RandomForest:  [0.80504283 0.8084208  0.81228134 0.8032332  0.80745566 0.81501146\n",
      " 0.80547846 0.8028237  0.80209967 0.80378907]\n",
      "Testing data accuracy for RandomForest:  [0.80125422 0.80752533 0.80511336 0.80173661 0.79739508 0.80733945\n",
      " 0.79575085 0.81409947 0.80685659 0.80251086]\n",
      "Training data accuracy for Decision Tree:  [0.77548558 0.77693329 0.78031126 0.7752443  0.77331403 0.78049958\n",
      " 0.77277664 0.78230964 0.7671051  0.77036322]\n",
      "Testing data accuracy for Decision Tree:  [0.72551857 0.75156778 0.74240232 0.74191992 0.73130728 0.72766779\n",
      " 0.71994206 0.73635925 0.73684211 0.72815065]\n",
      "Training data accuracy for Linear Regression :  [0.80733502 0.8179515  0.80673181 0.80335384 0.81541802 0.81139134\n",
      " 0.80403041 0.80620249 0.80789188 0.80125498]\n",
      "Testing data accuracy for Linear Regression  :  [0.80704293 0.81379643 0.80077183 0.80704293 0.80463097 0.80589087\n",
      " 0.78705939 0.8102366  0.81265089 0.81699662]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes model \n",
    "print(\"Training data accuracy for Naive Bayes: \",cross_val_score(bnb, X_train_skb, y_train_sample, cv=10))\n",
    "print(\"Testing data accuracy Naive Bayes: \",cross_val_score(bnb, X_test_skb, y_test_sample, cv=10))\n",
    "#KNN\n",
    "\n",
    "#print(\"Training data accuracy for KNN: \",cross_val_score(knn_model, X_train_pca, y_train, cv=10))\n",
    "#print(\"Testing data accuracy for KNN: \",cross_val_score(knn_model, X_test_pca, y_test, cv=10))\n",
    "#Random Forest\n",
    "print(\"Training data accuracy for RandomForest: \",cross_val_score(rfc, X_train_skb, y_train_sample, cv=10))\n",
    "print(\"Testing data accuracy for RandomForest: \",cross_val_score(rfc, X_test_skb, y_test_sample, cv=10))\n",
    "#Decision Tree\n",
    "print(\"Training data accuracy for Decision Tree: \",cross_val_score(decision_tree, X_train_skb, y_train_sample, cv=10))\n",
    "print(\"Testing data accuracy for Decision Tree: \",cross_val_score(decision_tree, X_test_skb, y_test_sample, cv=10))\n",
    "#SVC\n",
    "#print(\"Training data accuracy for SVC: \",cross_val_score(SVC(), X_train_pca, y_train, cv=6))\n",
    "#print(\"Testing data accuracy for SVC: \",cross_val_score(SVC(), X_test_pca, y_test, cv=6))\n",
    "#Linear Regression\n",
    "print(\"Training data accuracy for Linear Regression : \",cross_val_score(lr, X_train_skb, y_train_sample, cv=10))\n",
    "print(\"Testing data accuracy for Linear Regression  : \",cross_val_score(lr, X_test_skb, y_test_sample, cv=10))\n",
    "#GBR\n",
    "#print(\"Training data accuracy for GBR: \",cross_val_score(clf, X_train_pca, y_train, cv=10))\n",
    "#print(\"Testing data accuracy for GBR: \",cross_val_score(clf, X_test_pca, y_test, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually looks like we did good  on cross val with SKB than we did with our full feature set. There is no overfitting since we have derived our own features with the keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score for BNB : 0.7073864700171258\n",
      "AUC Score for Random Forest: 0.8317776339482007\n",
      "AUC Score for Decision Tree: 0.9983948360060639\n",
      "AUC Score for Linear Regression: 0.8089648167617285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print (\"AUC Score for BNB :\" ,roc_auc_score(y_train_sample, y_pred_bnb))\n",
    "#print (\"AUC Score for KNN:\" ,roc_auc_score(y_train, y_pred_knn))\n",
    "print (\"AUC Score for Random Forest:\" ,roc_auc_score(y_train_sample, y_pred_rfc))\n",
    "print (\"AUC Score for Decision Tree:\" ,roc_auc_score(y_train_sample, y_pred_dt))\n",
    "#print (\"AUC Score for SVM:\" ,roc_auc_score(y_train, y_pred_svm))\n",
    "print (\"AUC Score for Linear Regression:\" ,roc_auc_score(y_train_sample, y_pred_lr))\n",
    "#print (\"AUC Score for GBR:\" ,roc_auc_score(y_train, y_pred_gbr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC score is also good after doing SKB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29719 11766]\n",
      " [12485 28910]]\n",
      "Accuracy Score for BNB : 0.7073962355212355\n",
      "[[31404 10081]\n",
      " [ 3868 37527]]\n",
      "Accuracy Score for Random Forest : 0.8316964285714286\n",
      "[[41435    50]\n",
      " [   83 41312]]\n",
      "Accuracy Score for Decision Tree : 0.9983952702702703\n",
      "[[34019  7466]\n",
      " [ 8366 33029]]\n",
      "Accuracy Score for Linear Regression : 0.808976833976834\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "print(confusion_matrix(y_train_sample, y_pred_bnb))\n",
    "print('Accuracy Score for BNB :',accuracy_score(y_train_sample, y_pred_bnb))\n",
    "#print(confusion_matrix(y_train, y_pred_knn))\n",
    "#print('Accuracy Score for KNN :',accuracy_score(y_train, y_pred_knn))\n",
    "print(confusion_matrix(y_train_sample, y_pred_rfc))\n",
    "print('Accuracy Score for Random Forest :',accuracy_score(y_train_sample, y_pred_rfc))\n",
    "print(confusion_matrix(y_train_sample, y_pred_dt))\n",
    "print('Accuracy Score for Decision Tree :',accuracy_score(y_train_sample, y_pred_dt))\n",
    "#print(confusion_matrix(y_train, y_pred_svm))\n",
    "#print('Accuracy Score for SVM:',accuracy_score(y_train, y_pred_svm))\n",
    "print(confusion_matrix(y_train_sample, y_pred_lr))\n",
    "print('Accuracy Score for Linear Regression :',accuracy_score(y_train_sample, y_pred_lr))\n",
    "#print(confusion_matrix(y_train, y_pred_gbr))\n",
    "#print('Accuracy Score for GBR :',accuracy_score(y_train, y_pred_gbr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did faily well but this is just for positive  keywords in customer reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for RFC:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.76      0.82     41485\n",
      "          1       0.79      0.91      0.84     41395\n",
      "\n",
      "avg / total       0.84      0.83      0.83     82880\n",
      "\n",
      "Classification Report for DT:              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     41485\n",
      "          1       1.00      1.00      1.00     41395\n",
      "\n",
      "avg / total       1.00      1.00      1.00     82880\n",
      "\n",
      "Classification Report for LR:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.82      0.81     41485\n",
      "          1       0.82      0.80      0.81     41395\n",
      "\n",
      "avg / total       0.81      0.81      0.81     82880\n",
      "\n",
      "Classification Report for BNB:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.72      0.71     41485\n",
      "          1       0.71      0.70      0.70     41395\n",
      "\n",
      "avg / total       0.71      0.71      0.71     82880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "#print(\"Classification Report for KNN:\",classification_report(y_train, y_pred_knn))\n",
    "print(\"Classification Report for RFC:\",classification_report(y_train_sample, y_pred_rfc))\n",
    "print(\"Classification Report for DT:\",classification_report(y_train_sample, y_pred_dt))\n",
    "#print(\"Classification Report for SVM:\",classification_report(y_train, y_pred_svm))\n",
    "print(\"Classification Report for LR:\",classification_report(y_train_sample, y_pred_lr))\n",
    "#print(\"Classification Report for GBR:\",classification_report(y_train, y_pred_gbr))\n",
    "print(\"Classification Report for BNB:\",classification_report(y_train_sample, y_pred_bnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 score and recall score is very good.The score didnot change  when compared to PCA values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models have had good accuracy score.There are 3 models -KNN,SVC,GBS which are very slow.It has been more than 7 hours , then these ,models are still running.There is  no difference in performance between PCA and SKB.This is a classification problem so we have used classification models and have tried to get the accuracy to predict the outome.The end goal is to predict the right outcome so for that the accuracy should be considerably high.If we observe this above cross validation accuracy,f1 score,precision is very promising.As observed above, it does not make any difference if we use PCA or SKB, accuracy score and f1 score remained similar.This is what we had expected.We usually expect GBS to slowest of all since it runs in multiple cycle to get more accuracy but a little surpised with SVC and KNN.This problem is about predicting positive reviews where we have been presented the text and reviews of some customers and we are checking how accurate are the models in predicting the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
