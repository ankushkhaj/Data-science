{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import neural_network\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "consumer_key = '66n3K6o0GfkbLO4QEoqRCA2pZ'\n",
    "consumer_secret = 'T1fsUWei3jSvOehl9TmRqIuIhTGi0UEudOAeT04PMLcgodQzhS'\n",
    "access_token = '86377594-cpO17ciBmTRbHzKD0AikvyIgJg7Ohfj5klfTogjVh'\n",
    "access_token_secret = 'pMIxRtyha7XNeyBITlLwprYGwfl2DM4laCB2dUaDXCrCA'\n",
    " \n",
    "def  twitter_Access():\n",
    "     auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "     auth.set_access_token(access_token, access_token_secret)\n",
    "     api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "     return api\n",
    "extract = twitter_Access()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Twitter's API only allows for 200 records per call\n",
    "TWEETS_PER_CALL = 200 \n",
    "\n",
    "def save_tweets(screen_nm, tweets):\n",
    "    # 'a' appends to the .json we wrote with 'w' above\n",
    "    with open(f'C:\\\\Users\\\\Ankush\\\\Desktop\\\\{screen_nm}.json', 'a') as f:\n",
    "        for tweet in tweets:\n",
    "            json.dump([tweet.text, screen_nm], f); f.write('\\n')\n",
    "\n",
    "def get_tweets(screen_nm, desired_ct):\n",
    "    \n",
    "    #desired_ct is the number of tweets the user wants to include\n",
    "    tweet_list = []\n",
    "\n",
    "    #grab the current maximum tweet id for provided screen name, which will be the id of the first tweet stored in the object\n",
    "    tweets = extract.user_timeline(screen_name = screen_nm,count=200)\n",
    "    save_tweets(screen_nm, tweets)\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump = get_tweets('realDonaldTrump',200)\n",
    "clinton = get_tweets('HillaryClinton', 200)\n",
    "bernie = get_tweets('SenSanders', 200)\n",
    "obama = get_tweets('BarackObama', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = ['realDonaldTrump', 'HillaryClinton', 'SenSanders', 'BarackObama', 'MikeBloomberg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Combine all tweets into 1 list\n",
    "\n",
    "full_tweets = []\n",
    "for handle in handles:\n",
    "    with open('C:\\\\Users\\\\Ankush\\\\Desktop\\\\{}.json'.format(handle)) as f: \n",
    "        full_tweets.append([json.loads(line) for line in f])\n",
    "        \n",
    "#Only want the first 2100 from each author (retweets caused the full amount for each author to be < 3200, and each has at least\n",
    "#2100 non RTs)\n",
    "all_tweets = []\n",
    "for auth in full_tweets:\n",
    "    all_tweets.extend(auth[:1000])\n",
    "    \n",
    "#Store in dataframe\n",
    "tweet_df = pd.DataFrame(all_tweets, columns = ['Tweets', 'Author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>....This will be remembered as one of the most...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senate Democrats just voted against legislatio...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If a deal is made with China, our great Americ...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heading over to Vietnam for my meeting with Ki...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China Trade Deal (and more) in advanced stages...</td>\n",
       "      <td>realDonaldTrump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets           Author\n",
       "0  ....This will be remembered as one of the most...  realDonaldTrump\n",
       "1  Senate Democrats just voted against legislatio...  realDonaldTrump\n",
       "2  If a deal is made with China, our great Americ...  realDonaldTrump\n",
       "3  Heading over to Vietnam for my meeting with Ki...  realDonaldTrump\n",
       "4  China Trade Deal (and more) in advanced stages...  realDonaldTrump"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet_full=tweet_df['Tweets'].replace(r'--',' ').astype('str')\n",
    "tweet_full= ' '.join(tweet_df['Tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "tweet_full=nlp(tweet_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    allwords = [token.lemma_ for token in text if not token.is_stop and not token.is_punct and not token.like_url and \n",
    "                not token.like_num and not token.is_space and token.is_ascii and len(token.lemma_) > 1]\n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = bag_of_words(tweet_full)\n",
    "\n",
    "#Let's clean this up a bit \n",
    "replace_words = [r'/n', r'\\\\n', r'\\n', r'--', r'-', 'https:\\/\\/t\\.co\\/[\\w\\d]+', r'amp', r'&', r'w/', \"'s\"]\n",
    "bow = [re.sub('|'.join(replace_words), '', word) for word in bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe with counts of our bag of words as features \n",
    "def bow_features(sentences, bow):\n",
    "    \n",
    "    partsofspeechlist = []\n",
    "    ent_list = []\n",
    "    \n",
    "    for sent in sentences[0]:\n",
    "        for token in sent:\n",
    "                partsofspeechlist.append(token.pos_)\n",
    "\n",
    "    parts = (set(partsofspeechlist))\n",
    "\n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns = set(list(bow) + list(parts)))\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, bow] = 0\n",
    "    #df['tweet_length'] = 0\n",
    "    #df['tweet_punct_count'] = 0 \n",
    "    #df['upper_case'] = 0 \n",
    "    df.loc[:, parts] = 0\n",
    "    \n",
    "    for i, sent in enumerate(sentences[0]):\n",
    "\n",
    "        tweet_len = 0    \n",
    "        num_punct = 0\n",
    "        \n",
    "        # Populate the row with word counts (only include the ones from our bag of words)\n",
    "        words = [token.lemma_ for token in sent\n",
    "                 if token.lemma_ in bow]\n",
    "        \n",
    "        #\"words\" collects all bow matches from each sentence\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        #Add part of speech counter and tweet length\n",
    "        for token in sent:\n",
    "            df.loc[i, token.pos_] += 1\n",
    "            if not token.is_punct:\n",
    "                tweet_len += 1\n",
    "            else:\n",
    "                num_punct += 1\n",
    "            \n",
    "        #Create a feature for all CAPS words  \n",
    "           # if ((str(token).isupper()) & (len(str(token)) > 1)):\n",
    "             #   df.loc[i, 'upper_case'] += 1\n",
    "\n",
    "\n",
    "        #df.loc[i, 'tweet_length'] = tweet_len\n",
    "        #df.loc[i, 'tweet_punct_count'] = num_punct\n",
    "\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize our tweets using spaCy (for more interesting features later on)\n",
    "all_tweets_nlp = []\n",
    "all_tweets_nlp.append([nlp(tweet[0]) for tweet in all_tweets])\n",
    "all_tweets_nlp.append([nlp(tweet[1]) for tweet in all_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n"
     ]
    }
   ],
   "source": [
    "df = bow_features(all_tweets_nlp, bow)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "X = np.array(df.drop(['text_sentence', 'text_source'],1))\n",
    "\n",
    "#y needs to be str or int, cannot be spacy doc, so convert here\n",
    "Y = df['text_source'].apply(lambda x: str(x))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#print('Training set score:', rfc.score(X_train, y_train))\n",
    "#print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_grid={'max_depth':[3,4,5],'max_features': [5]}\n",
    "\n",
    "grid_class_rfc=model_selection.GridSearchCV(rfc,dt_grid,cv=6)\n",
    "grid_class_rfc.fit(X_train_pca, y_train_sample)\n",
    "results = grid_class_rfc.cv_results_\n",
    "print(results.get('params'))\n",
    "print(results.get('mean_test_score'))\n",
    "print(results.get('mean_train_score'))\n",
    "print(\"Best Hyper Parameters:\\n\",grid_class_rfc.best_params_)\n",
    "print(grid_class_rfc.best_score_)\n",
    "final_model = grid_class_rfc.best_estimator_\n",
    "print(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the best pararmeters to the model\n",
    "rfc = ensemble.RandomForestClassifier(max_depth=5,max_features= 5)\n",
    "rfc.fit(X_train_pca, y_train_sample)\n",
    "y_pred_rfc = rfc.predict(X_train_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validaton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data accuracy for Random Forest: \",cross_val_score(rfc, X_train_pca, y_train_sample, cv=10))\n",
    "print(\"Testing data accuracy for Random Forest: \",cross_val_score(rfc, X_test_pca, y_test_sample, cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print (\"AUC Score for Random Forest :\" ,roc_auc_score(y_train_sample, y_pred_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "print(confusion_matrix(y_train_sample, y_pred_rfc))\n",
    "print('Accuracy Score for Random Forest :',accuracy_score(y_train_sample, y_pred_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report for Random Forest:\",classification_report(y_train_sample, y_pred_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Confusion matrix for model random forest performance visualization\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(X.corr(),linewidth = 1,annot= True, annot_kws={\"size\": 9})\n",
    "plt.title('Variable Correlation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
